{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5541095",
   "metadata": {},
   "source": [
    "## 1.Imports & Configuration 🛠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b3e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 1: Imports & Configuration ###\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm # The notebook-friendly progress bar\n",
    "\n",
    "# --- 🕷️ PRAWLER CONFIGURATION ---\n",
    "# PASTE YOUR REDDIT API CREDENTIALS HERE\n",
    "CLIENT_ID = \"cqUf_9T9v0tHisDODAz2rA\"\n",
    "CLIENT_SECRET = \"V5Mt83tpp1vTyBiPlW5vF6xqOLzedw\"\n",
    "USER_AGENT = \"Python:PeerHiveV2-Scraper:v1.0 (by /u/Significant-Luck6567)\" # Change to your Reddit username\n",
    "\n",
    "# --- 🎯 TARGETS ---\n",
    "SUBREDDITS = ['gradschool', 'college', 'PhD', 'csMajors', 'Professors', 'medicalschool']\n",
    "KEYWORDS = ['burnout', 'overwhelmed', 'exhausted', 'stressed', \"can't cope\", 'dropping out', 'imposter syndrome', 'completely drained', 'running on fumes', 'lost the passion']\n",
    "POST_LIMIT_PER_KEYWORD = 750\n",
    "\n",
    "# --- 💾 OUTPUT ---\n",
    "OUTPUT_FILENAME = \"reddit_burnout_anonymized_for_annotation.csv\"\n",
    "\n",
    "print(\"✅ Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a1ae3",
   "metadata": {},
   "source": [
    "## 2.The PII Scrubber 🧼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb40732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PII Scrubber function defined.\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 2: The PII Scrubber Function ###\n",
    "\n",
    "def scrub_pii(text: str) -> str:\n",
    "    \"\"\"Anonymizes text by removing potential PII.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Regex to find and replace u/username or /u/username patterns, case-insensitive\n",
    "    text = re.sub(r'(?:\\s|^)(?:u/|/u/)([a-zA-Z0-9_-]+)', ' [USER]', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "print(\"✅ PII Scrubber function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a555f",
   "metadata": {},
   "source": [
    "## 3.Connect to Reddit API 🔌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55736d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Reddit as: None\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 3: Connect to Reddit API ###\n",
    "\n",
    "reddit = None\n",
    "if not CLIENT_ID or not CLIENT_SECRET:\n",
    "    print(\"🛑 ERROR: Please fill in your Reddit API credentials in the first code cell.\")\n",
    "else:\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET,\n",
    "            user_agent=USER_AGENT,\n",
    "        )\n",
    "        print(f\"✅ Successfully connected to Reddit as: {reddit.user.me()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"🔥 Reddit connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe70f8",
   "metadata": {},
   "source": [
    "## 4.The Collection Engine 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2f20cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hunting in subreddits: r/gradschool+college+PhD+csMajors+Professors+medicalschool\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544d455ad55a4e10ae56113214662cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing keywords:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Collection Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 4: The Collection Engine ###\n",
    "\n",
    "collected_posts = []\n",
    "if reddit:\n",
    "    subreddit_query = \"+\".join(SUBREDDITS)\n",
    "    print(f\"Hunting in subreddits: r/{subreddit_query}\\n\")\n",
    "\n",
    "    for keyword in tqdm(KEYWORDS, desc=\"Processing keywords\"):\n",
    "        search_query = f'selftext:\"{keyword}\"'\n",
    "        search_results = reddit.subreddit(subreddit_query).search(\n",
    "            search_query,\n",
    "            limit=POST_LIMIT_PER_KEYWORD,\n",
    "            sort='relevance',\n",
    "            time_filter='all'\n",
    "        )\n",
    "\n",
    "        for post in search_results:\n",
    "            if post.selftext:\n",
    "                collected_posts.append({\n",
    "                    'id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'body': post.selftext,\n",
    "                    'subreddit': str(post.subreddit),\n",
    "                    'url': post.url\n",
    "                })\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\\n--- Collection Complete ---\")\n",
    "else:\n",
    "    print(\"⚠️ Reddit client not connected. Please run the previous cell successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b33513",
   "metadata": {},
   "source": [
    "## 5.Process, Anonymize, and Save 💾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80466958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1471 posts in total.\n",
      "Found 1390 unique posts after de-duplication.\n",
      "\n",
      "--- Running Anonymization Protocol ---\n",
      "✅ PII scrubbing complete.\n",
      "\n",
      "💾 Gold standard dataset ready for annotation! Saved to 'reddit_burnout_anonymized_for_annotation.csv'\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 5: Process, Anonymize, and Save ###\n",
    "\n",
    "if not collected_posts:\n",
    "    print(\"⚠️ No posts were collected. Cannot proceed.\")\n",
    "else:\n",
    "    df_raw = pd.DataFrame(collected_posts)\n",
    "    print(f\"Collected {len(df_raw)} posts in total.\")\n",
    "\n",
    "    df_raw.drop_duplicates(subset='id', inplace=True)\n",
    "    print(f\"Found {len(df_raw)} unique posts after de-duplication.\")\n",
    "\n",
    "    print(\"\\n--- Running Anonymization Protocol ---\")\n",
    "    df_anonymized = pd.DataFrame()\n",
    "    df_anonymized['id'] = df_raw['id']\n",
    "    df_anonymized['clean_title'] = df_raw['title'].apply(scrub_pii)\n",
    "    df_anonymized['clean_body'] = df_raw['body'].apply(scrub_pii)\n",
    "    df_anonymized['subreddit'] = df_raw['subreddit']\n",
    "    df_anonymized['url'] = df_raw['url']\n",
    "    print(\"✅ PII scrubbing complete.\")\n",
    "\n",
    "    df_anonymized.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
    "    print(f\"\\n💾 Gold standard dataset ready for annotation! Saved to '{OUTPUT_FILENAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ded44f",
   "metadata": {},
   "source": [
    "## 6.Verify the Output ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3a931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1myx7vx</td>\n",
       "      <td>My career is over. What a relief.</td>\n",
       "      <td>I am starting my 18th year, and am allowed to ...</td>\n",
       "      <td>Professors</td>\n",
       "      <td>https://www.reddit.com/r/Professors/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1jctt1m</td>\n",
       "      <td>Anyone else just… not want to grade?</td>\n",
       "      <td>I know, I know… it’s part of the job. But with...</td>\n",
       "      <td>Professors</td>\n",
       "      <td>https://www.reddit.com/r/Professors/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1lrpnvw</td>\n",
       "      <td>First year as a lecturer here. Student absente...</td>\n",
       "      <td>I'm new to teaching and I genuinely care about...</td>\n",
       "      <td>Professors</td>\n",
       "      <td>https://www.reddit.com/r/Professors/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1lciz2j</td>\n",
       "      <td>Successfully defended just an hour ago!</td>\n",
       "      <td>After spending 5 years in a STEM PhD program f...</td>\n",
       "      <td>PhD</td>\n",
       "      <td>https://i.redd.it/zf3wxlopm77f1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1lf0t2n</td>\n",
       "      <td>Emergency medicine sounds too good to be true ...</td>\n",
       "      <td>EDIT: Thanks to all the ED attendings for lett...</td>\n",
       "      <td>medicalschool</td>\n",
       "      <td>https://www.reddit.com/r/medicalschool/comment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        clean_title  \\\n",
       "0  1myx7vx                  My career is over. What a relief.   \n",
       "1  1jctt1m               Anyone else just… not want to grade?   \n",
       "2  1lrpnvw  First year as a lecturer here. Student absente...   \n",
       "3  1lciz2j            Successfully defended just an hour ago!   \n",
       "4  1lf0t2n  Emergency medicine sounds too good to be true ...   \n",
       "\n",
       "                                          clean_body      subreddit  \\\n",
       "0  I am starting my 18th year, and am allowed to ...     Professors   \n",
       "1  I know, I know… it’s part of the job. But with...     Professors   \n",
       "2  I'm new to teaching and I genuinely care about...     Professors   \n",
       "3  After spending 5 years in a STEM PhD program f...            PhD   \n",
       "4  EDIT: Thanks to all the ED attendings for lett...  medicalschool   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/Professors/comments/1...  \n",
       "1  https://www.reddit.com/r/Professors/comments/1...  \n",
       "2  https://www.reddit.com/r/Professors/comments/1...  \n",
       "3               https://i.redd.it/zf3wxlopm77f1.jpeg  \n",
       "4  https://www.reddit.com/r/medicalschool/comment...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Cell 6: Verify the Output ###\n",
    "\n",
    "if os.path.exists(OUTPUT_FILENAME):\n",
    "    df_final = pd.read_csv(OUTPUT_FILENAME)\n",
    "    display(df_final.head())\n",
    "else:\n",
    "    print(f\"File '{OUTPUT_FILENAME}' not found. Make sure the previous cells ran correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d67c1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1390</td>\n",
       "      <td>1390</td>\n",
       "      <td>1390</td>\n",
       "      <td>1390</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1390</td>\n",
       "      <td>1385</td>\n",
       "      <td>1390</td>\n",
       "      <td>6</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1myx7vx</td>\n",
       "      <td>Recent CS Grad? Lets lock in together and bag ...</td>\n",
       "      <td>I am starting my 18th year, and am allowed to ...</td>\n",
       "      <td>PhD</td>\n",
       "      <td>https://www.reddit.com/r/Professors/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                        clean_title  \\\n",
       "count      1390                                               1390   \n",
       "unique     1390                                               1385   \n",
       "top     1myx7vx  Recent CS Grad? Lets lock in together and bag ...   \n",
       "freq          1                                                  2   \n",
       "\n",
       "                                               clean_body subreddit  \\\n",
       "count                                                1390      1390   \n",
       "unique                                               1390         6   \n",
       "top     I am starting my 18th year, and am allowed to ...       PhD   \n",
       "freq                                                    1       368   \n",
       "\n",
       "                                                      url  \n",
       "count                                                1390  \n",
       "unique                                               1390  \n",
       "top     https://www.reddit.com/r/Professors/comments/1...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7495d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1390 raw, de-duplicated posts.\n",
      "Shuffling dataset and taking a reproducible sample of 600...\n",
      "✅ Final dataset created with 600 posts.\n",
      "💾 Your final, shuffled dataset is ready. Feed 'data_for_annotation.csv' to The Guillotine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_DATA_FILE = \"reddit_burnout_anonymized_for_annotation.csv\" # The output from your PRAWler script\n",
    "FINAL_DATA_FILE = \"data_for_annotation.csv\" # The file you will feed to The Guillotine\n",
    "TARGET_SIZE = 600\n",
    "RANDOM_STATE = 42 # The answer to life, the universe, and reproducible science.\n",
    "\n",
    "# --- SCRIPT ---\n",
    "if not os.path.exists(RAW_DATA_FILE):\n",
    "    print(f\"🔥 FATAL ERROR: Raw data file '{RAW_DATA_FILE}' not found.\")\n",
    "    print(\"Please run the PRAWler script first to generate the raw, de-duplicated data.\")\n",
    "else:\n",
    "    df = pd.read_csv(RAW_DATA_FILE)\n",
    "    print(f\"Loaded {len(df)} raw, de-duplicated posts.\")\n",
    "\n",
    "    if len(df) < TARGET_SIZE:\n",
    "        print(f\"⚠️ Warning: You have fewer than {TARGET_SIZE} posts. Using all {len(df)} posts.\")\n",
    "        final_df = df\n",
    "    else:\n",
    "        # The one and only shuffle. Reproducible every time.\n",
    "        print(f\"Shuffling dataset and taking a reproducible sample of {TARGET_SIZE}...\")\n",
    "        final_df = df.sample(n=TARGET_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    print(f\"✅ Final dataset created with {len(final_df)} posts.\")\n",
    "\n",
    "    final_df.to_csv(FINAL_DATA_FILE, index=False)\n",
    "    print(f\"💾 Your final, shuffled dataset is ready. Feed '{FINAL_DATA_FILE}' to The Guillotine.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30dd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
